{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e6ddfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from io import BytesIO, StringIO\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b4c526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_subset_toxicity.csv\", 'rb') as f:\n",
    "    rawdata = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4135694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>url</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>toxicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shashi tharoor set to run for congress preside...</td>\n",
       "      <td>extremely valid points but i believe he has th...</td>\n",
       "      <td>/r/india/comments/xif8wm/shashi_tharoor_set_to...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yogi government puts kolkata s image as part o...</td>\n",
       "      <td>even if the ad was designed by the newspaper t...</td>\n",
       "      <td>/r/india/comments/pmn9o3/yogi_government_puts_...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>an attempt to address the list of simplified e...</td>\n",
       "      <td>diverse population including muslims welcoming...</td>\n",
       "      <td>/r/india/comments/ebdeup/an_attempt_to_address...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you guys noticing what s happening in sri lanka</td>\n",
       "      <td>what a joke they didn t create any propaganda...</td>\n",
       "      <td>/r/india/comments/tt1ryh/you_guys_noticing_wha...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>just got abused on the train by a hyper nation...</td>\n",
       "      <td>ohoo bahut bura laga ye sunke ki aap undergarm...</td>\n",
       "      <td>/r/india/comments/rh2kcs/just_got_abused_on_th...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         clean_title  \\\n",
       "0  shashi tharoor set to run for congress preside...   \n",
       "1  yogi government puts kolkata s image as part o...   \n",
       "2  an attempt to address the list of simplified e...   \n",
       "3   you guys noticing what s happening in sri lanka    \n",
       "4  just got abused on the train by a hyper nation...   \n",
       "\n",
       "                                       clean_comment  \\\n",
       "0  extremely valid points but i believe he has th...   \n",
       "1  even if the ad was designed by the newspaper t...   \n",
       "2  diverse population including muslims welcoming...   \n",
       "3   what a joke they didn t create any propaganda...   \n",
       "4  ohoo bahut bura laga ye sunke ki aap undergarm...   \n",
       "\n",
       "                                                 url  avg_score  toxicity  \n",
       "0  /r/india/comments/xif8wm/shashi_tharoor_set_to...       -1.0      0.00  \n",
       "1  /r/india/comments/pmn9o3/yogi_government_puts_...       -1.0      0.00  \n",
       "2  /r/india/comments/ebdeup/an_attempt_to_address...       -1.0      0.00  \n",
       "3  /r/india/comments/tt1ryh/you_guys_noticing_wha...        0.0      0.00  \n",
       "4  /r/india/comments/rh2kcs/just_got_abused_on_th...       -2.0      0.66  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_rawdata = str(rawdata, 'utf-8', errors='replace')\n",
    "text_df = pd.read_csv(StringIO(str_rawdata))\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f97eec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_score</th>\n",
       "      <th>toxicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.043000</td>\n",
       "      <td>0.206687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.951202</td>\n",
       "      <td>0.325896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         avg_score     toxicity\n",
       "count  3000.000000  3000.000000\n",
       "mean     -0.043000     0.206687\n",
       "std       0.951202     0.325896\n",
       "min      -2.000000     0.000000\n",
       "25%      -1.000000     0.000000\n",
       "50%       0.000000     0.000000\n",
       "75%       0.000000     0.500000\n",
       "max       2.000000     1.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55028617",
   "metadata": {},
   "source": [
    "## Inter annotator agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9b44bae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agreement: 0.971; by-chance: 0.314; kappa score: 0.958\n"
     ]
    }
   ],
   "source": [
    "def inter_annotator_agreement(judge1, judeg2):\n",
    "    def prob_chance(label):\n",
    "        j1_chance = ([judge_1[i]==label for i in range(3000)].count(True)) / 3000.0\n",
    "        j2_chance = ([judge_2[i]==label for i in range(3000)].count(True)) / 3000.0\n",
    "        p_e = j1_chance * j2_chance\n",
    "        return p_e\n",
    "    \n",
    "    p_a = ( [abs( judge_1[i]-judge_2[i] ) <= 1 for i in range(3000)].count(True) ) / 3000.0\n",
    "    p_e = sum(prob_chance(label) for label in [-2, -1, 0, 1, 2])\n",
    "    kappa = (p_a - p_e) / (1 - p_e)\n",
    "    print(f\"agreement: {p_a:.3f}; by-chance: {p_e:.3f}; kappa score: {kappa:.3f}\")\n",
    "\n",
    "judge_1 = text_df['label1'].to_numpy()\n",
    "judge_2 = text_df['label2'].to_numpy()\n",
    "\n",
    "inter_annotator_agreement(judge_1, judge_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7a725e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.816"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentiment(score):\n",
    "    if score < 0: return -1\n",
    "    if score > 0: return 1\n",
    "    return 0\n",
    "\n",
    "judge_1 = list(map(sentiment, text_df['label1'].to_numpy()))\n",
    "judge_2 = list(map(sentiment, text_df['label2'].to_numpy()))\n",
    "\n",
    "# from sklearn.metrics import cohen_kappa_score\n",
    "# score = cohen_kappa_score(judge_1, judge_2)\n",
    "# print(score)\n",
    "\n",
    "p_a = ([judge_1[i]==judge_2[i] for i in range(3000)].count(True)) / 3000.0\n",
    "p_a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844a0a3e",
   "metadata": {},
   "source": [
    "## Text Cleaning\n",
    " - Remove all quotes of previous comments (starts with \">\" and ends with newline)\n",
    " - Remove special characters other than ,.'\"?!\n",
    " - Remove links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc537de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_quotations(text):\n",
    "    temp = re.sub(r'>.*?\\n', '', text)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e371e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_punc = re.compile('[^a-zA-Z0-9\\s]')\n",
    "# print(clean_punc.sub('', reviews[0][\"text\"][0].lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d3f7728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\">The blockade was done by your own people.\\n\\nWhat a joke. \\n\\n>The simple truth is that your communist party aligned with China and created propaganda against India.\\n\\nThey didn't create any propaganda. India helped their civil war. They are puppet of India. Majority of them have no courage to talk anything about India. \\n\\nPeople in India die due to Koshi that's why India wants to construct another larger dam so that only Nepali would die.\\n\\nAnd another simple explanation, when India initiated a road in Lipulekh Kalapani area (which according to Sugauli treaty) is part of Nepal. India never talked about it with Nepal. Because India dissolved the treaty without Nepal's agreement. \\n\\nWhen every argument ends, we have common culture is your ultimate sword.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df['comment'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a5311f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWhat a joke. \\n\\n\\nThey didn't create any propaganda. India helped their civil war. They are puppet of India. Majority of them have no courage to talk anything about India. \\n\\nPeople in India die due to Koshi that's why India wants to construct another larger dam so that only Nepali would die.\\n\\nAnd another simple explanation, when India initiated a road in Lipulekh Kalapani area (which according to Sugauli treaty) is part of Nepal. India never talked about it with Nepal. Because India dissolved the treaty without Nepal's agreement. \\n\\nWhen every argument ends, we have common culture is your ultimate sword.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_quotations(text_df['comment'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a69bab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_url(text):\n",
    "    temp = re.sub(r'(http(s)?:\\/\\/.)?(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+\\.~#?&\\/=]*)', '', text)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b056eb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_chars(text):\n",
    "    temp = re.sub(r'[^a-zA-Z0-9\\s\\.,?!\\'\\\"]', ' ', text)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c737d415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(text):\n",
    "    temp = text.lower()\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0e216fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    temp = clean_quotations(text)\n",
    "    temp = clean_url(temp)\n",
    "    temp = \n",
    "    temp = re.sub(r'\\s+', ' ', temp)  # replace multiple spaces with single space\n",
    "    return temp.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23ecb563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What a joke. They didn't create any propaganda. India helped their civil war. They are puppet of India. Majority of them have no courage to talk anything about India. People in India die due to Koshi that's why India wants to construct another larger dam so that only Nepali would die. And another simple explanation, when India initiated a road in Lipulekh Kalapani area which according to Sugauli treaty is part of Nepal. India never talked about it with Nepal. Because India dissolved the treaty without Nepal's agreement. When every argument ends, we have common culture is your ultimate sword.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(text_df['comment'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5d8e37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt at more elaborate translation, for those who might be interested:\n",
      "\n",
      "> They are plastering one movie's posters all over the place.\n",
      "> \n",
      "> The whole of BJP cadre is involved in plastering such posters.\n",
      "> \n",
      "> This is why you came in poltics? To plaster... what will you tell you children at home when they ask what do you do for living? - *I plaster movie's posters.*\n",
      "> \n",
      "> They are saying that Kashmir Files should be tax free, why not just upload it on Youtube? It'll be all free free.\n",
      "> \n",
      "> Why are you getting it tax free? Just ask Vivek Agnihotri to put it all on Youtube, it will all be free for everybody to watch.\n",
      "> \n",
      "> I read something in the newspaper yesterday - there's a Haryana BJP MLA who said that he will get a free screening of the movie held in some park - immediately, Vivek posted on twitter addressing Manohar Lal Khattar about this free screening and asking him to tell that MLA to pay for that screening. \n",
      "> \n",
      "> Listen, some guys are earning crores out of Kashmiri Pandits' tragedy, and you guys are plastering posters on the walls for them. Open your eyes! What has become of you people? \n",
      "> \n",
      "> After eight years of ruling a country, if that country's Prime Minister has to bend his knees in front of Vivek Agnihotri, it means that PM hasn't done any work in all those years.\n",
      "\n",
      "**Edit**: Full video here - https://youtube.com/watch?v=6zLEV34OZKA\n"
     ]
    }
   ],
   "source": [
    "print(text_df['comment'][14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbf1965e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Attempt at more elaborate translation, for those who might be interested Edit Full video here'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(text_df['comment'][14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdff8368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>url</th>\n",
       "      <th>avg_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shashi Tharoor Set To Run For Congress Preside...</td>\n",
       "      <td>Extremely valid points but I believe he has th...</td>\n",
       "      <td>/r/india/comments/xif8wm/shashi_tharoor_set_to...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yogi government puts Kolkata's image as part o...</td>\n",
       "      <td>Even if the ad was designed by the newspaper t...</td>\n",
       "      <td>/r/india/comments/pmn9o3/yogi_government_puts_...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An attempt to address the list of \"simplified\"...</td>\n",
       "      <td>Diverse population including Muslims. Welcomin...</td>\n",
       "      <td>/r/india/comments/ebdeup/an_attempt_to_address...</td>\n",
       "      <td>-1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You guys noticing what's happening in Sri Lanka?</td>\n",
       "      <td>What a joke. They didn't create any propaganda...</td>\n",
       "      <td>/r/india/comments/tt1ryh/you_guys_noticing_wha...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got abused on the train by a hyper nation...</td>\n",
       "      <td>Ohoo bahut bura laga ye sunke ki aap Undergarm...</td>\n",
       "      <td>/r/india/comments/rh2kcs/just_got_abused_on_th...</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         clean_title  \\\n",
       "0  Shashi Tharoor Set To Run For Congress Preside...   \n",
       "1  Yogi government puts Kolkata's image as part o...   \n",
       "2  An attempt to address the list of \"simplified\"...   \n",
       "3   You guys noticing what's happening in Sri Lanka?   \n",
       "4  Just got abused on the train by a hyper nation...   \n",
       "\n",
       "                                       clean_comment  \\\n",
       "0  Extremely valid points but I believe he has th...   \n",
       "1  Even if the ad was designed by the newspaper t...   \n",
       "2  Diverse population including Muslims. Welcomin...   \n",
       "3  What a joke. They didn't create any propaganda...   \n",
       "4  Ohoo bahut bura laga ye sunke ki aap Undergarm...   \n",
       "\n",
       "                                                 url  avg_score  \n",
       "0  /r/india/comments/xif8wm/shashi_tharoor_set_to...       -1.0  \n",
       "1  /r/india/comments/pmn9o3/yogi_government_puts_...       -1.0  \n",
       "2  /r/india/comments/ebdeup/an_attempt_to_address...       -1.5  \n",
       "3  /r/india/comments/tt1ryh/you_guys_noticing_wha...        0.0  \n",
       "4  /r/india/comments/rh2kcs/just_got_abused_on_th...       -2.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df = pd.DataFrame()\n",
    "clean_df['clean_title'] = text_df.apply(lambda x: clean_text(x['submission_title']), axis=1)\n",
    "clean_df['clean_comment'] = text_df.apply(lambda x: clean_text(x['comment']), axis=1)\n",
    "clean_df[['url', 'avg_score']] = text_df[['url', 'avg_score']]\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0220480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv(\"cleaned_subset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2b59db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>url</th>\n",
       "      <th>avg_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shashi Tharoor Set To Run For Congress Preside...</td>\n",
       "      <td>Extremely valid points but I believe he has th...</td>\n",
       "      <td>/r/india/comments/xif8wm/shashi_tharoor_set_to...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yogi government puts Kolkata's image as part o...</td>\n",
       "      <td>Even if the ad was designed by the newspaper t...</td>\n",
       "      <td>/r/india/comments/pmn9o3/yogi_government_puts_...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An attempt to address the list of \"simplified\"...</td>\n",
       "      <td>Diverse population including Muslims. Welcomin...</td>\n",
       "      <td>/r/india/comments/ebdeup/an_attempt_to_address...</td>\n",
       "      <td>-1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You guys noticing what's happening in Sri Lanka?</td>\n",
       "      <td>What a joke. They didn't create any propaganda...</td>\n",
       "      <td>/r/india/comments/tt1ryh/you_guys_noticing_wha...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got abused on the train by a hyper nation...</td>\n",
       "      <td>Ohoo bahut bura laga ye sunke ki aap Undergarm...</td>\n",
       "      <td>/r/india/comments/rh2kcs/just_got_abused_on_th...</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         clean_title  \\\n",
       "0  Shashi Tharoor Set To Run For Congress Preside...   \n",
       "1  Yogi government puts Kolkata's image as part o...   \n",
       "2  An attempt to address the list of \"simplified\"...   \n",
       "3   You guys noticing what's happening in Sri Lanka?   \n",
       "4  Just got abused on the train by a hyper nation...   \n",
       "\n",
       "                                       clean_comment  \\\n",
       "0  Extremely valid points but I believe he has th...   \n",
       "1  Even if the ad was designed by the newspaper t...   \n",
       "2  Diverse population including Muslims. Welcomin...   \n",
       "3  What a joke. They didn't create any propaganda...   \n",
       "4  Ohoo bahut bura laga ye sunke ki aap Undergarm...   \n",
       "\n",
       "                                                 url  avg_score  \n",
       "0  /r/india/comments/xif8wm/shashi_tharoor_set_to...       -1.0  \n",
       "1  /r/india/comments/pmn9o3/yogi_government_puts_...       -1.0  \n",
       "2  /r/india/comments/ebdeup/an_attempt_to_address...       -1.5  \n",
       "3  /r/india/comments/tt1ryh/you_guys_noticing_wha...        0.0  \n",
       "4  /r/india/comments/rh2kcs/just_got_abused_on_th...       -2.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df = pd.read_csv(\"cleaned_subset.csv\")\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e502510e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8bdc3a93",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c470e677",
   "metadata": {},
   "source": [
    "### Rule Based Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "de5ffd70",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tejas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\tejas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tejas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tejas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd259e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "right = \"\"\"right wing, RW, authority, hierarchy, order, duty, tradition, reaction, nationalism, conservative, right-libertarian, \\\n",
    "neoconservative, imperialist, monarchist, fascist, reactionaries, traditionalist, traditional, death penalty, \\\n",
    "religion, Bhajpa, BJP, Shiv Sena, RSS, MNS, Sanatan, dharm, Hindutva, Islamophobia, Narendra, Modi, Amit, Shah, \\\n",
    "mandir, ram, valmiki, ramayan, Bharatiya, Janata, Democratic Alliance, NDA, AIADMK, Janta Dal, bhakt, CAA, NRC, hindu majority, \\\n",
    "hindu unity, hindu pride, nationalist, sangh, sanghi, yogi\"\"\".lower().split(', ')\n",
    "left = \"\"\"left wing, LW, leftists, freedom, equality, fraternity, rights, progress, reform, internationalism, anarchist, communist, socialist, \\\n",
    "democratic socialist, social democrat, left-libertarian, progressive, social, liberal, Congress, UPA, \\\n",
    "INC, Aam, aadmi, AAP, CPI, CPI(M), Welfare, Protectionism, Commies, Rahul, gandhi, indira, yatra, arvind, kejriwal, \\\n",
    "libby, libbies, sjw, libtard, hinduphobia, LGBTQ, masjid, pappu, christian, muslim, secular, minority, minorities, Shashi, Tharoor\"\"\".lower().split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7a73476a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_title</th>\n",
       "      <th>comment</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>url</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>cleaned_comment</th>\n",
       "      <th>cleaned_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shashi Tharoor Set To Run For Congress Preside...</td>\n",
       "      <td>Extremely valid points but I believe he has th...</td>\n",
       "      <td>ip5g6vu</td>\n",
       "      <td>/r/india/comments/xif8wm/shashi_tharoor_set_to...</td>\n",
       "      <td>2qh1q</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>extremely valid points but i believe he has th...</td>\n",
       "      <td>shashi tharoor set to run for congress preside...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yogi government puts Kolkata's image as part o...</td>\n",
       "      <td>Even if the ad was designed by the newspaper -...</td>\n",
       "      <td>hcontm8</td>\n",
       "      <td>/r/india/comments/pmn9o3/yogi_government_puts_...</td>\n",
       "      <td>2qh1q</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>even if the ad was designed by the newspaper t...</td>\n",
       "      <td>yogi government puts kolkata s image as part o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An attempt to address the list of \"simplified\"...</td>\n",
       "      <td>Diverse population including Muslims. Welcomin...</td>\n",
       "      <td>fbhlv40</td>\n",
       "      <td>/r/india/comments/ebdeup/an_attempt_to_address...</td>\n",
       "      <td>2qh1q</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>diverse population including muslims welcoming...</td>\n",
       "      <td>an attempt to address the list of simplified e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You guys noticing what's happening in Sri Lanka?</td>\n",
       "      <td>&gt;The blockade was done by your own people.\\n\\n...</td>\n",
       "      <td>i2yrud3</td>\n",
       "      <td>/r/india/comments/tt1ryh/you_guys_noticing_wha...</td>\n",
       "      <td>2qh1q</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>what a joke they didn t create any propaganda...</td>\n",
       "      <td>you guys noticing what s happening in sri lanka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got abused on the train by a hyper-nation...</td>\n",
       "      <td>Ohoo bahut bura laga ye sunke ki aap Undergarm...</td>\n",
       "      <td>hooi92k</td>\n",
       "      <td>/r/india/comments/rh2kcs/just_got_abused_on_th...</td>\n",
       "      <td>2qh1q</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>ohoo bahut bura laga ye sunke ki aap undergarm...</td>\n",
       "      <td>just got abused on the train by a hyper nation...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    submission_title  \\\n",
       "0  Shashi Tharoor Set To Run For Congress Preside...   \n",
       "1  Yogi government puts Kolkata's image as part o...   \n",
       "2  An attempt to address the list of \"simplified\"...   \n",
       "3   You guys noticing what's happening in Sri Lanka?   \n",
       "4  Just got abused on the train by a hyper-nation...   \n",
       "\n",
       "                                             comment comment_id  \\\n",
       "0  Extremely valid points but I believe he has th...    ip5g6vu   \n",
       "1  Even if the ad was designed by the newspaper -...    hcontm8   \n",
       "2  Diverse population including Muslims. Welcomin...    fbhlv40   \n",
       "3  >The blockade was done by your own people.\\n\\n...    i2yrud3   \n",
       "4  Ohoo bahut bura laga ye sunke ki aap Undergarm...    hooi92k   \n",
       "\n",
       "                                                 url subreddit_id  label1  \\\n",
       "0  /r/india/comments/xif8wm/shashi_tharoor_set_to...        2qh1q      -1   \n",
       "1  /r/india/comments/pmn9o3/yogi_government_puts_...        2qh1q      -1   \n",
       "2  /r/india/comments/ebdeup/an_attempt_to_address...        2qh1q      -1   \n",
       "3  /r/india/comments/tt1ryh/you_guys_noticing_wha...        2qh1q       0   \n",
       "4  /r/india/comments/rh2kcs/just_got_abused_on_th...        2qh1q      -2   \n",
       "\n",
       "   label2  avg_score                                    cleaned_comment  \\\n",
       "0      -1       -1.0  extremely valid points but i believe he has th...   \n",
       "1      -1       -1.0  even if the ad was designed by the newspaper t...   \n",
       "2      -2       -1.0  diverse population including muslims welcoming...   \n",
       "3       0        0.0   what a joke they didn t create any propaganda...   \n",
       "4      -2       -2.0  ohoo bahut bura laga ye sunke ki aap undergarm...   \n",
       "\n",
       "                                       cleaned_title  \n",
       "0  shashi tharoor set to run for congress preside...  \n",
       "1  yogi government puts kolkata s image as part o...  \n",
       "2  an attempt to address the list of simplified e...  \n",
       "3   you guys noticing what s happening in sri lanka   \n",
       "4  just got abused on the train by a hyper nation...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to clean the text\n",
    "def clean(text):\n",
    "    res_txt = []\n",
    "    for line in text.split('\\n'):\n",
    "        if not line.startswith(\">\"): \n",
    "            res_txt.append(line.lower())\n",
    "    text = \"\\n\".join(res_txt)\n",
    "    \n",
    "    text = re.sub(r'(http(s)?:\\/\\/.)?(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+\\.~#?&\\/=]*)', ' ', text)\n",
    "    text = re.sub('[^A-Za-z]+', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "text_df['cleaned_comment'] = text_df['comment'].apply(clean)\n",
    "text_df['cleaned_title'] = text_df['submission_title'].apply(clean)\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ec7ac580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>url</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>sentiment_comment</th>\n",
       "      <th>sentiment_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shashi tharoor set to run for congress preside...</td>\n",
       "      <td>extremely valid points but i believe he has th...</td>\n",
       "      <td>/r/india/comments/xif8wm/shashi_tharoor_set_to...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yogi government puts kolkata s image as part o...</td>\n",
       "      <td>even if the ad was designed by the newspaper t...</td>\n",
       "      <td>/r/india/comments/pmn9o3/yogi_government_puts_...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>an attempt to address the list of simplified e...</td>\n",
       "      <td>diverse population including muslims welcoming...</td>\n",
       "      <td>/r/india/comments/ebdeup/an_attempt_to_address...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you guys noticing what s happening in sri lanka</td>\n",
       "      <td>what a joke they didn t create any propaganda...</td>\n",
       "      <td>/r/india/comments/tt1ryh/you_guys_noticing_wha...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>just got abused on the train by a hyper nation...</td>\n",
       "      <td>ohoo bahut bura laga ye sunke ki aap undergarm...</td>\n",
       "      <td>/r/india/comments/rh2kcs/just_got_abused_on_th...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         clean_title  \\\n",
       "0  shashi tharoor set to run for congress preside...   \n",
       "1  yogi government puts kolkata s image as part o...   \n",
       "2  an attempt to address the list of simplified e...   \n",
       "3   you guys noticing what s happening in sri lanka    \n",
       "4  just got abused on the train by a hyper nation...   \n",
       "\n",
       "                                       clean_comment  \\\n",
       "0  extremely valid points but i believe he has th...   \n",
       "1  even if the ad was designed by the newspaper t...   \n",
       "2  diverse population including muslims welcoming...   \n",
       "3   what a joke they didn t create any propaganda...   \n",
       "4  ohoo bahut bura laga ye sunke ki aap undergarm...   \n",
       "\n",
       "                                                 url  avg_score  toxicity  \\\n",
       "0  /r/india/comments/xif8wm/shashi_tharoor_set_to...       -1.0      0.00   \n",
       "1  /r/india/comments/pmn9o3/yogi_government_puts_...       -1.0      0.00   \n",
       "2  /r/india/comments/ebdeup/an_attempt_to_address...       -1.0      0.00   \n",
       "3  /r/india/comments/tt1ryh/you_guys_noticing_wha...        0.0      0.00   \n",
       "4  /r/india/comments/rh2kcs/just_got_abused_on_th...       -2.0      0.66   \n",
       "\n",
       "   sentiment_comment  sentiment_title  \n",
       "0                  0                0  \n",
       "1                  0                0  \n",
       "2                  0                0  \n",
       "3                  0                0  \n",
       "4                  0                0  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word frequency for comment\n",
    "from nltk import FreqDist\n",
    "def word_count(lemmatized_text):\n",
    "    words = lemmatized_text.split()\n",
    "    temp=zip(*[words[i:] for i in range(0,2)])\n",
    "    words.extend([' '.join(t) for t in temp])\n",
    "    fdist = FreqDist(words)\n",
    "    return fdist\n",
    "\n",
    "# Calculate sentiment of word frequency comment\n",
    "def sentiment_count(freq_dist):\n",
    "    total = 0\n",
    "    for term, freq in freq_dist.items():\n",
    "        if term in left: \n",
    "            total -= freq\n",
    "        if term in right: \n",
    "            total += freq\n",
    "    return total\n",
    "\n",
    "left = list(map(str.strip, left))\n",
    "right = list(map(str.strip, right))\n",
    "text_df['wordCount_comment'] = text_df['clean_comment'].apply(word_count)\n",
    "text_df['wordCount_title'] = text_df['clean_title'].apply(word_count)\n",
    "text_df['sentiment_comment'] = text_df['wordCount_comment'].apply(sentiment_count)\n",
    "text_df['sentiment_title'] = text_df['wordCount_title'].apply(sentiment_count)\n",
    "\n",
    "del text_df['wordCount_comment']\n",
    "del text_df['wordCount_title']\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5188aaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00      1423\n",
      "           1       0.53      1.00      0.69      1577\n",
      "\n",
      "    accuracy                           0.53      3000\n",
      "   macro avg       0.76      0.50      0.35      3000\n",
      "weighted avg       0.75      0.53      0.36      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def label(avg_score):\n",
    "    if avg_score < -0.25: return \"Left\"\n",
    "    if avg_score > 0.25: return \"Right\"\n",
    "    return \"Neutral\"\n",
    "\n",
    "true_labels = text_df['avg_score'].apply(label)\n",
    "pred_labels = text_df['sentiment_comment'].apply(label)\n",
    "\n",
    "report = classification_report(true_labels, pred_labels)\n",
    "print(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c34a9b4",
   "metadata": {},
   "source": [
    "### Toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c119ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "base_api = \"http://sentic.net/api/en/%s.py?text=%s\"\n",
    "polarity_key = \"zfCm4Hq4p0lgOJNU\"\n",
    "absa_key = \"xSK04b5TqYH2\"\n",
    "toxicity_key = \"TLHqwt0xWpvVG\"\n",
    "\n",
    "# Calculate toxicity of comment\n",
    "def toxicity(text):\n",
    "    x = requests.get(base_api % (toxicity_key, text))\n",
    "    try:\n",
    "        raw_value = float(x.text.strip('%\\n'))/100\n",
    "        return raw_value\n",
    "    except:\n",
    "        print(\"Error\")\n",
    "    return 0\n",
    "\n",
    "try:\n",
    "    for i, row in text_df.iterrows():\n",
    "        if np.isnan(row['toxicity']):\n",
    "            row['toxicity'] = toxicity(row['clean_comment'])\n",
    "            text_df.iloc[i] = row\n",
    "\n",
    "    text_df.to_csv('train_subset_toxicity.csv', index=False)\n",
    "\n",
    "except:\n",
    "    print(\"Error\")\n",
    "    text_df.to_csv('train_subset_toxicity_incomplete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edcb5e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.32      0.29      0.30       773\n",
      "     Neutral       0.67      0.67      0.67      1577\n",
      "       Right       0.27      0.31      0.29       650\n",
      "\n",
      "    accuracy                           0.49      3000\n",
      "   macro avg       0.42      0.42      0.42      3000\n",
      "weighted avg       0.49      0.49      0.49      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def label(avg_score):\n",
    "    if avg_score < -0.25: return \"Left\"\n",
    "    if avg_score > 0.25: return \"Right\"\n",
    "    return \"Neutral\"\n",
    "\n",
    "def infer(sentiment_comment, sentiment_title, toxicity):\n",
    "    if toxicity >= 0.5:\n",
    "        if sentiment_comment!=0: sentiment_comment = -1 * sentiment_comment\n",
    "        else: sentiment_comment = -1 * sentiment_title\n",
    "\n",
    "    return label(sentiment_comment)\n",
    "\n",
    "true_labels = text_df['avg_score'].apply(label)\n",
    "pred_labels = text_df.apply(lambda x: infer(x['sentiment_comment'], x['sentiment_title'], x['toxicity']), axis=1)\n",
    "\n",
    "report = classification_report(true_labels, pred_labels)\n",
    "print(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3097543f",
   "metadata": {},
   "source": [
    "### Random Forest Classifier & SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fbf9da57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(avg_score):\n",
    "    if avg_score < -0.25: return \"Left\"\n",
    "    if avg_score > 0.25: return \"Right\"\n",
    "    return \"Neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "03356092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.52      0.30      0.38       250\n",
      "     Neutral       0.62      0.89      0.73       546\n",
      "       Right       0.60      0.26      0.36       225\n",
      "\n",
      "    accuracy                           0.61      1021\n",
      "   macro avg       0.58      0.48      0.49      1021\n",
      "weighted avg       0.59      0.61      0.56      1021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Split the data into input features (text and float) and labels\n",
    "X = text_df[['clean_comment', 'toxicity']]\n",
    "y = text_df['avg_score'].apply(label)\n",
    "\n",
    "# Define the preprocessing steps for the text and float features\n",
    "text_transformer = TfidfVectorizer(ngram_range=(3,7), analyzer='char_wb')\n",
    "\n",
    "# Combine the preprocessing steps using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', text_transformer, 'clean_comment')\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "# Define the pipeline for the random forest classifier\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(kernel='linear', C=1))\n",
    "])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.34, random_state=123)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Compute the precision, recall, and F1 score of the predictions\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4de29c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.52      0.34      0.41       250\n",
      "     Neutral       0.64      0.90      0.75       546\n",
      "       Right       0.65      0.27      0.38       225\n",
      "\n",
      "    accuracy                           0.62      1021\n",
      "   macro avg       0.61      0.50      0.51      1021\n",
      "weighted avg       0.62      0.62      0.59      1021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Split the data into input features (text and float) and labels\n",
    "X = text_df[['clean_comment', 'toxicity']]\n",
    "y = text_df['avg_score'].apply(label)\n",
    "\n",
    "# Define the preprocessing steps for the text and float features\n",
    "text_transformer = TfidfVectorizer(max_features=20_000, ngram_range=(3,7), analyzer='char_wb')\n",
    "\n",
    "# Combine the preprocessing steps using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', text_transformer, 'clean_comment')\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "# Define the pipeline for the random forest classifier\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100))\n",
    "])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.34, random_state=123)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Compute the precision, recall, and F1 score of the predictions\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "003b334b",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ef161639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(avg_score):\n",
    "    if avg_score < -0.25: return 0\n",
    "    if avg_score > 0.25: return 2\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a80a687c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.35      0.42       250\n",
      "           1       0.64      0.89      0.74       546\n",
      "           2       0.57      0.23      0.33       225\n",
      "\n",
      "    accuracy                           0.61      1021\n",
      "   macro avg       0.58      0.49      0.50      1021\n",
      "weighted avg       0.60      0.61      0.57      1021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Split the data into input features (text and float) and labels\n",
    "X = text_df[['clean_comment', 'toxicity']]\n",
    "y = text_df['avg_score'].apply(label)\n",
    "\n",
    "# Define the preprocessing steps for the text and float features\n",
    "text_transformer = TfidfVectorizer(ngram_range=(3,7), analyzer='char_wb')\n",
    "\n",
    "# Combine the preprocessing steps using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', text_transformer, 'clean_comment')\n",
    "    ])\n",
    "\n",
    "# Define the pipeline for the random forest classifier\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', xgb.XGBClassifier(learning_rate=0.05))\n",
    "])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.34, random_state=123)\n",
    "\n",
    "# Train the pipeline using GridSearchCV\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Get the best pipeline and evaluate it on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Compute the precision, recall, and F1 score of the predictions\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5dd950da",
   "metadata": {},
   "source": [
    "### Two Layer Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70da68da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, 59)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right = \"\"\"right wing, RW, authority, hierarchy, order, duty, tradition, reaction, nationalism, conservative, right-libertarian, \\\n",
    "neoconservative, imperialist, monarchist, fascist, reactionaries, traditionalist, traditional, death penalty, \\\n",
    "religion, Bhajpa, BJP, Shiv Sena, RSS, MNS, Sanatan, dharm, Hindutva, Islamophobia, Narendra, Modi, Amit, Shah, \\\n",
    "mandir, ram, valmiki, ramayan, Bharatiya, Janata, Democratic Alliance, NDA, AIADMK, Janta Dal, bhakt, CAA, NRC, hindu majority, \\\n",
    "hindu unity, hindu pride, nationalist, sangh, sanghi, yogi, brahmin, brahman, smriti irani, hindu rashtra, jai shri ram, \\\n",
    "pm cares, pmcares, adani, hindu\"\"\".lower()\n",
    "left = \"\"\"left wing, LW, leftists, freedom, equality, fraternity, rights, progress, reform, internationalism, anarchist, communist, socialist, \\\n",
    "democratic socialist, social democrat, left-libertarian, progressive, social, liberal, western, Congress, UPA, RG, mamata, \\\n",
    "Aam, aadmi, AAP, CPI, Welfare, Protectionism, Commies, Rahul, gandhi, indira, yatra, arvind, kejriwal, inclusivity, \\\n",
    "libby, libbies, sjw, libtard, hinduphobia, LGBTQ, masjid, pappu, christian, muslim, secular, minority, minorities, Shashi, Tharoor, \\\n",
    "gay, lesbian, transgender, trans, reservation, abrahamic\"\"\".lower()\n",
    "\n",
    "right_terms = set(right.split(', '))\n",
    "left_terms = set(left.split(', '))\n",
    "len(right_terms), len(left_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "29768b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-class labels\n",
    "# Left (0), Neutral (1), Right (2)\n",
    "def label(avg_score):\n",
    "    if avg_score < -0.25: return 0\n",
    "    if avg_score > 0.25: return 2\n",
    "    return 1\n",
    "\n",
    "# Neutral (1) vs non-neutral (0)\n",
    "def label1(label):\n",
    "    if label in [0, 2]: return 0\n",
    "    return 1\n",
    "\n",
    "# Left (0) vs right (1)\n",
    "def label2(label):\n",
    "    if label == 2: return 1\n",
    "    return 0\n",
    "\n",
    "# Convert step 2 labels to 3-class predictions\n",
    "def convLabel(label):\n",
    "    if label==1: return 2\n",
    "    return 0\n",
    "\n",
    "def pass_through(x):\n",
    "    return np.array(x).astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0a20417",
   "metadata": {},
   "source": [
    "#### Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "411feb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\tejas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "def feature_extraction(clean_text, terms):\n",
    "    clean_text = clean_text.lower()\n",
    "    features = []\n",
    "    for t in terms:\n",
    "        features.append(len(re.findall(r'\\b'+t, clean_text)))\n",
    "\n",
    "    return sum(features)\n",
    "\n",
    "def add_sentiment_features(clean_text):\n",
    "    # appends sentiment scores onto extracted features\n",
    "    temp = []\n",
    "    temp.append(vader.polarity_scores(clean_text)['compound'])\n",
    "    tb = TextBlob(clean_text)\n",
    "    temp.append(tb.sentiment.polarity)\n",
    "    temp.append(tb.sentiment.subjectivity)\n",
    "    return temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "514ceb87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>url</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>sentiment_comment</th>\n",
       "      <th>sentiment_title</th>\n",
       "      <th>left_features</th>\n",
       "      <th>right_features</th>\n",
       "      <th>sentiment_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shashi tharoor set to run for congress preside...</td>\n",
       "      <td>extremely valid points but i believe he has th...</td>\n",
       "      <td>/r/india/comments/xif8wm/shashi_tharoor_set_to...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yogi government puts kolkata s image as part o...</td>\n",
       "      <td>even if the ad was designed by the newspaper t...</td>\n",
       "      <td>/r/india/comments/pmn9o3/yogi_government_puts_...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>an attempt to address the list of simplified e...</td>\n",
       "      <td>diverse population including muslims welcoming...</td>\n",
       "      <td>/r/india/comments/ebdeup/an_attempt_to_address...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you guys noticing what s happening in sri lanka</td>\n",
       "      <td>what a joke they didn t create any propaganda...</td>\n",
       "      <td>/r/india/comments/tt1ryh/you_guys_noticing_wha...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.9264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>just got abused on the train by a hyper nation...</td>\n",
       "      <td>ohoo bahut bura laga ye sunke ki aap undergarm...</td>\n",
       "      <td>/r/india/comments/rh2kcs/just_got_abused_on_th...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         clean_title  \\\n",
       "0  shashi tharoor set to run for congress preside...   \n",
       "1  yogi government puts kolkata s image as part o...   \n",
       "2  an attempt to address the list of simplified e...   \n",
       "3   you guys noticing what s happening in sri lanka    \n",
       "4  just got abused on the train by a hyper nation...   \n",
       "\n",
       "                                       clean_comment  \\\n",
       "0  extremely valid points but i believe he has th...   \n",
       "1  even if the ad was designed by the newspaper t...   \n",
       "2  diverse population including muslims welcoming...   \n",
       "3   what a joke they didn t create any propaganda...   \n",
       "4  ohoo bahut bura laga ye sunke ki aap undergarm...   \n",
       "\n",
       "                                                 url  avg_score  toxicity  \\\n",
       "0  /r/india/comments/xif8wm/shashi_tharoor_set_to...       -1.0      0.00   \n",
       "1  /r/india/comments/pmn9o3/yogi_government_puts_...       -1.0      0.00   \n",
       "2  /r/india/comments/ebdeup/an_attempt_to_address...       -1.0      0.00   \n",
       "3  /r/india/comments/tt1ryh/you_guys_noticing_wha...        0.0      0.00   \n",
       "4  /r/india/comments/rh2kcs/just_got_abused_on_th...       -2.0      0.66   \n",
       "\n",
       "   sentiment_comment  sentiment_title  left_features  right_features  \\\n",
       "0                  0                0              1               0   \n",
       "1                  0                0              0               0   \n",
       "2                  0                0              4               0   \n",
       "3                  0                0              0               0   \n",
       "4                  0                0              1               0   \n",
       "\n",
       "   sentiment_features  \n",
       "0              0.5941  \n",
       "1             -0.2960  \n",
       "2              0.5719  \n",
       "3             -0.9264  \n",
       "4              0.7595  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = text_df.copy()\n",
    "temp_df['left_features'] = temp_df.apply(lambda x: feature_extraction(x['clean_comment'], left_terms), axis=1)\n",
    "temp_df['right_features'] = temp_df.apply(lambda x: feature_extraction(x['clean_comment'], right_terms), axis=1)\n",
    "temp_df['sentiment_features'] = temp_df.apply(lambda x: add_sentiment_features(x['clean_comment']), axis=1)\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07814043",
   "metadata": {},
   "source": [
    "#### 2 step model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "33fceadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-neutral       0.69      0.67      0.68       475\n",
      "     Neutral       0.72      0.74      0.73       546\n",
      "\n",
      "    accuracy                           0.71      1021\n",
      "   macro avg       0.71      0.71      0.71      1021\n",
      "weighted avg       0.71      0.71      0.71      1021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X = temp_df[['clean_comment', 'toxicity', 'left_features', 'right_features', 'sentiment_features']]\n",
    "y = temp_df['avg_score'].apply(label)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.34, random_state=123)\n",
    "\n",
    "X_train_step1 = X_train.copy()\n",
    "X_test_step1 = X_test.copy()\n",
    "y_train_step1 = y_train.apply(label1)\n",
    "y_test_step1 = y_test.apply(label1)\n",
    "\n",
    "# Define the preprocessing steps for the text and float features\n",
    "text_transformer = TfidfVectorizer(max_features=20_000, analyzer='char_wb', ngram_range=(3,7))\n",
    "\n",
    "# Combine the preprocessing steps using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', text_transformer, 'clean_comment'),\n",
    "        ('num', StandardScaler(), ['left_features', 'right_features'])\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "# Step 1: Predict neutral or non-neutral\n",
    "pipeline_step1 = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('clf', xgb.XGBClassifier(max_depth=4, learning_rate=0.05))\n",
    "])\n",
    "pipeline_step1.fit(X_train_step1, y_train_step1)\n",
    "\n",
    "# Predictions of step 1\n",
    "y_pred_step1 = pipeline_step1.predict(X_test_step1)\n",
    "\n",
    "# Print the classification report for the 1st predictions\n",
    "print(classification_report(y_test_step1, y_pred_step1, target_names=['Non-neutral', 'Neutral']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1534e98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.45      0.56      0.50       250\n",
      "     Neutral       0.72      0.74      0.73       546\n",
      "       Right       0.53      0.36      0.42       225\n",
      "\n",
      "    accuracy                           0.61      1021\n",
      "   macro avg       0.57      0.55      0.55      1021\n",
      "weighted avg       0.61      0.61      0.61      1021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# rus = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# Step 2: Predict left or right based on the output of step 1\n",
    "X_train_step2 = X_train[y_train != 1]\n",
    "X_train_step2 = X_train_step2[['clean_comment']]#, 'left_features', 'right_features']]\n",
    "y_train_step2 = y_train[y_train != 1].apply(label2)\n",
    "# X_train_step2, y_train_step2 = rus.fit_resample(X_train_step2, y_train_step2)\n",
    "\n",
    "# Define the preprocessing steps for the text and float features\n",
    "text_transformer = TfidfVectorizer(max_features=20_000, analyzer='char_wb', ngram_range=(3,7))\n",
    "\n",
    "# Combine the preprocessing steps using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', text_transformer, 'clean_comment')\n",
    "        # ('num', StandardScaler(), ['left_features', 'right_features'])\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "# Step 2: Predict left or right\n",
    "pipeline_step2 = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('clf', xgb.XGBClassifier(max_depth=10, learning_rate=0.05))\n",
    "])\n",
    "pipeline_step2.fit(X_train_step2, y_train_step2)\n",
    "\n",
    "# Predictions of step 2\n",
    "X_test_step2 = X_test[y_pred_step1 != 1]\n",
    "X_test_step2 = X_test_step2[['clean_comment']]#, 'left_features', 'right_features']]\n",
    "y_test_step2 = y_test[y_pred_step1 != 1]\n",
    "y_pred_step2 = pipeline_step2.predict(X_test_step2)\n",
    "\n",
    "# Combine the predictions of step 1 and step 2\n",
    "y_pred_step2 = np.array(list(map(convLabel, y_pred_step2)))\n",
    "y_pred = y_pred_step1.copy()\n",
    "y_pred[y_pred_step1 != 1] = y_pred_step2\n",
    "\n",
    "# Print the classification report for the combined predictions\n",
    "print(classification_report(y_test, y_pred, target_names=['Left', 'Neutral', 'Right']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19ca0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24a146cb",
   "metadata": {},
   "source": [
    "### BERT-based classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "985802f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ace181e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>url</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shashi Tharoor Set To Run For Congress Preside...</td>\n",
       "      <td>Extremely valid points but I believe he has th...</td>\n",
       "      <td>/r/india/comments/xif8wm/shashi_tharoor_set_to...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yogi government puts Kolkata's image as part o...</td>\n",
       "      <td>Even if the ad was designed by the newspaper t...</td>\n",
       "      <td>/r/india/comments/pmn9o3/yogi_government_puts_...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An attempt to address the list of \"simplified\"...</td>\n",
       "      <td>Diverse population including Muslims. Welcomin...</td>\n",
       "      <td>/r/india/comments/ebdeup/an_attempt_to_address...</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>-0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You guys noticing what's happening in Sri Lanka?</td>\n",
       "      <td>What a joke. They didn't create any propaganda...</td>\n",
       "      <td>/r/india/comments/tt1ryh/you_guys_noticing_wha...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got abused on the train by a hyper nation...</td>\n",
       "      <td>Ohoo bahut bura laga ye sunke ki aap Undergarm...</td>\n",
       "      <td>/r/india/comments/rh2kcs/just_got_abused_on_th...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         clean_title  \\\n",
       "0  Shashi Tharoor Set To Run For Congress Preside...   \n",
       "1  Yogi government puts Kolkata's image as part o...   \n",
       "2  An attempt to address the list of \"simplified\"...   \n",
       "3   You guys noticing what's happening in Sri Lanka?   \n",
       "4  Just got abused on the train by a hyper nation...   \n",
       "\n",
       "                                       clean_comment  \\\n",
       "0  Extremely valid points but I believe he has th...   \n",
       "1  Even if the ad was designed by the newspaper t...   \n",
       "2  Diverse population including Muslims. Welcomin...   \n",
       "3  What a joke. They didn't create any propaganda...   \n",
       "4  Ohoo bahut bura laga ye sunke ki aap Undergarm...   \n",
       "\n",
       "                                                 url  avg_score  target  \n",
       "0  /r/india/comments/xif8wm/shashi_tharoor_set_to...       -1.0   -0.50  \n",
       "1  /r/india/comments/pmn9o3/yogi_government_puts_...       -1.0   -0.50  \n",
       "2  /r/india/comments/ebdeup/an_attempt_to_address...       -1.5   -0.75  \n",
       "3  /r/india/comments/tt1ryh/you_guys_noticing_wha...        0.0    0.00  \n",
       "4  /r/india/comments/rh2kcs/just_got_abused_on_th...       -2.0   -1.00  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df['target'] = clean_df['avg_score']/2\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d3df4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"google/muril-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "89a8b9f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   104,  96009,   1691,  38666,   6810,   1363,    148,   8994,   1157,\n",
       "           1207,   1108,  19794,   4394,   1192,   1113,   6952,   1750,  16762,\n",
       "          10743,   1341,   1121,   7783,   1108,   1936,  31551,  24418,  51325,\n",
       "           4382,  60648,    121,   7154,    119,    148,   1678,   5526,   1725,\n",
       "          44165,   6127,   2959, 183153, 159371,   1207,    172,  12521,   1109,\n",
       "          49676,   2219,   2733,   9610,   1113,   9989,   1147,    121,    105,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(clean_df['clean_comment'][0], padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43853d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb7845bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MuRILbase(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.2):\n",
    "\n",
    "        super(MuRILbase, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('google/muril-base-cased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.l1 = nn.Linear(768, 200)\n",
    "        self.l2 = nn.Linear(200, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        modules = [self.bert.embeddings, *self.bert.encoder.layer[:-3]]  # freeze all but last few\n",
    "        for module in modules:\n",
    "            for param in module.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        x = self.dropout(pooled_output)\n",
    "        x = self.l1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.tanh(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f401798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/muril-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MuRILbase(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(197285, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (l1): Linear(in_features=768, out_features=200, bias=True)\n",
       "  (l2): Linear(in_features=200, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (tanh): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MuRILbase()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb14ee59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0133],\n",
      "        [-0.0146],\n",
      "        [-0.0140]], device='cuda:0')\n",
      "tensor([-0.0133, -0.0146, -0.0140], device='cuda:0')\n",
      "tensor([-0.5000, -0.5000, -0.7500], device='cuda:0')\n",
      "0.3380824625492096\n",
      "0.33807727694511414\n"
     ]
    }
   ],
   "source": [
    "# testing with a few inputs\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "temp_batch = list(clean_df['clean_comment'][0:3])\n",
    "temp_targets = torch.tensor(list(clean_df['target'][0:3])).to(device)\n",
    "temp = tokenizer(temp_batch, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n",
    "input_ids = temp['input_ids'].to(device)\n",
    "attention_mask = temp['attention_mask'].to(device)\n",
    "with torch.no_grad():\n",
    "    preds = model(input_ids, attention_mask)\n",
    "#     loss = criterion(preds, temp_targets)\n",
    "    print(preds)\n",
    "    print(preds.squeeze(1))\n",
    "    print(temp_targets)\n",
    "#     print(loss.item())\n",
    "    loss = criterion(preds.squeeze(1), temp_targets)\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca9db02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5b49ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, train_df, num_epochs, batch_size, lr=0.001):\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    losses = []\n",
    "    \n",
    "    inputs = train_df.copy()\n",
    "    for _ in range(num_epochs):\n",
    "        inputs = inputs.sample(frac=1).reset_index(drop=True)  # shuffle order\n",
    "        for i in range(int(np.ceil(len(inputs)/batch_size))):\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # getting inputs\n",
    "            batch = list(inputs['clean_comment'][i:i+batch_size])\n",
    "            targets = torch.tensor(list(inputs['target'][i:i+batch_size])).to(device)\n",
    "            temp = tokenizer(batch, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n",
    "            input_ids, attention_mask = temp['input_ids'].to(device), temp['attention_mask'].to(device)\n",
    "            \n",
    "            # training model\n",
    "            preds = model(input_ids, attention_mask).squeeze(1)\n",
    "            batch_loss = criterion(preds, targets)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(batch_loss.item())\n",
    "            \n",
    "            # excplicitly delete variables in cuda\n",
    "            del batch, targets, temp, input_ids, attention_mask, preds, batch_loss\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39428f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/muril-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = MuRILbase().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f404159",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |     906 MB |    2080 MB |  602567 MB |  601661 MB |\n",
      "|       from large pool |     905 MB |    2077 MB |  602101 MB |  601195 MB |\n",
      "|       from small pool |       1 MB |       4 MB |     466 MB |     465 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |     906 MB |    2080 MB |  602567 MB |  601661 MB |\n",
      "|       from large pool |     905 MB |    2077 MB |  602101 MB |  601195 MB |\n",
      "|       from small pool |       1 MB |       4 MB |     466 MB |     465 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     998 MB |    2910 MB |    2910 MB |    1912 MB |\n",
      "|       from large pool |     994 MB |    2904 MB |    2904 MB |    1910 MB |\n",
      "|       from small pool |       4 MB |       6 MB |       6 MB |       2 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   93370 KB |  168966 KB |  270720 MB |  270629 MB |\n",
      "|       from large pool |   90368 KB |  166912 KB |  270094 MB |  270006 MB |\n",
      "|       from small pool |    3002 KB |    3002 KB |     625 MB |     622 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     205    |     464    |   70201    |   69996    |\n",
      "|       from large pool |      75    |     171    |   44386    |   44311    |\n",
      "|       from small pool |     130    |     295    |   25815    |   25685    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     205    |     464    |   70201    |   69996    |\n",
      "|       from large pool |      75    |     171    |   44386    |   44311    |\n",
      "|       from small pool |     130    |     295    |   25815    |   25685    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      22    |      70    |      70    |      48    |\n",
      "|       from large pool |      20    |      67    |      67    |      47    |\n",
      "|       from small pool |       2    |       3    |       3    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      30    |      50    |   34931    |   34901    |\n",
      "|       from large pool |      24    |      43    |   24920    |   24896    |\n",
      "|       from small pool |       6    |      11    |   10011    |   10005    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e41b528a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model, losses = train_loop(model, clean_df, num_epochs=5, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74c2ec5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x27ab2e46850>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i*5/len(losses) for i in range(len(losses))], losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab6ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
